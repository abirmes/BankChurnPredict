{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d997b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c125bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad7cd038",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] Le fichier spécifié est introuvable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(spark.version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abirm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abirm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abirm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abirm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abirm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\java_gateway.py:104\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m     proc = Popen(command, **popen_kwargs)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     proc = \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc.poll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abirm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:1039\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1035\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_mode:\n\u001b[32m   1036\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1037\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1039\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1049\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[32m   1050\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m.stdin, \u001b[38;5;28mself\u001b[39m.stdout, \u001b[38;5;28mself\u001b[39m.stderr)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abirm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:1554\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[39m\n\u001b[32m   1552\u001b[39m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[32m   1553\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1554\u001b[39m     hp, ht, pid, tid = \u001b[43m_winapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1555\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[32m   1556\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1557\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1558\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1559\u001b[39m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1563\u001b[39m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[32m   1564\u001b[39m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1567\u001b[39m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[32m   1568\u001b[39m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[32m   1569\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_pipe_fds(p2cread, p2cwrite,\n\u001b[32m   1570\u001b[39m                          c2pread, c2pwrite,\n\u001b[32m   1571\u001b[39m                          errread, errwrite)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 2] Le fichier spécifié est introuvable"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c8460",
   "metadata": {},
   "source": [
    "# Verification de version spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f1e808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.1\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('dataset.csv', header=True , inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888b6e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "|RowNumber|CustomerId|  Surname|CreditScore|Geography|Gender|Age|Tenure|  Balance|NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|\n",
      "+---------+----------+---------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "|        1|  15634602| Hargrave|        619|   France|Female| 42|     2|        0|            1|        1|             1|      101348.88|     1|\n",
      "|        2|  15647311|     Hill|        608|    Spain|Female| 41|     1| 83807.86|            1|        0|             1|      112542.58|     0|\n",
      "|        3|  15619304|     Onio|        502|   France|Female| 42|     8| 159660.8|            3|        1|             0|      113931.57|     1|\n",
      "|        4|  15701354|     Boni|        699|   France|Female| 39|     1|        0|            2|        0|             0|       93826.63|     0|\n",
      "|        5|  15737888| Mitchell|        850|    Spain|Female| 43|     2|125510.82|            1|        1|             1|        79084.1|     0|\n",
      "|        6|  15574012|      Chu|        645|    Spain|  Male| 44|     8|113755.78|            2|        1|             0|      149756.71|     1|\n",
      "|        7|  15592531| Bartlett|        822|   France|  Male| 50|     7|        0|            2|        1|             1|        10062.8|     0|\n",
      "|        8|  15656148|   Obinna|        376|  Germany|Female| 29|     4|115046.74|            4|        1|             0|      119346.88|     1|\n",
      "|        9|  15792365|       He|        501|   France|  Male| 44|     4|142051.07|            2|        0|             1|        74940.5|     0|\n",
      "|       10|  15592389|       H?|        684|   France|  Male| 27|     2|134603.88|            1|        1|             1|       71725.73|     0|\n",
      "|       11|  15767821|   Bearce|        528|   France|  Male| 31|     6|102016.72|            2|        0|             0|       80181.12|     0|\n",
      "|       12|  15737173|  Andrews|        497|    Spain|  Male| 24|     3|        0|            2|        1|             0|       76390.01|     0|\n",
      "|       13|  15632264|      Kay|        476|   France|Female| 34|    10|        0|            2|        1|             0|       26260.98|     0|\n",
      "|       14|  15691483|     Chin|        549|   France|Female| 25|     5|        0|            2|        0|             0|      190857.79|     0|\n",
      "|       15|  15600882|    Scott|        635|    Spain|Female| 35|     7|        0|            2|        1|             1|       65951.65|     0|\n",
      "|       16|  15643966|  Goforth|        616|  Germany|  Male| 45|     3|143129.41|            2|        0|             1|       64327.26|     0|\n",
      "|       17|  15737452|    Romeo|        653|  Germany|  Male| 58|     1|132602.88|            1|        1|             0|        5097.67|     1|\n",
      "|       18|  15788218|Henderson|        549|    Spain|Female| 24|     9|        0|            2|        1|             1|       14406.41|     0|\n",
      "|       19|  15661507|  Muldrow|        587|    Spain|  Male| 45|     6|        0|            1|        0|             0|      158684.81|     0|\n",
      "|       20|  15568982|      Hao|        726|   France|Female| 24|     6|        0|            2|        1|             1|       54724.03|     0|\n",
      "+---------+----------+---------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da528a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RowNumber: integer (nullable = true)\n",
      " |-- CustomerId: integer (nullable = true)\n",
      " |-- Surname: string (nullable = true)\n",
      " |-- CreditScore: integer (nullable = true)\n",
      " |-- Geography: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Tenure: integer (nullable = true)\n",
      " |-- Balance: double (nullable = true)\n",
      " |-- NumOfProducts: integer (nullable = true)\n",
      " |-- HasCrCard: integer (nullable = true)\n",
      " |-- IsActiveMember: integer (nullable = true)\n",
      " |-- EstimatedSalary: double (nullable = true)\n",
      " |-- Exited: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccddcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-------+-----------------+---------+------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+\n",
      "|summary|         RowNumber|       CustomerId|Surname|      CreditScore|Geography|Gender|               Age|            Tenure|          Balance|     NumOfProducts|          HasCrCard|     IsActiveMember|  EstimatedSalary|             Exited|\n",
      "+-------+------------------+-----------------+-------+-----------------+---------+------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+\n",
      "|  count|             10000|            10000|  10000|            10000|    10000| 10000|             10000|             10000|            10000|             10000|              10000|              10000|            10000|              10000|\n",
      "|   mean|            5000.5|  1.56909405694E7|   NULL|         650.5288|     NULL|  NULL|           38.9218|            5.0128|76485.88928799961|            1.5302|             0.7055|             0.5151|100090.2398809998|             0.2037|\n",
      "| stddev|2886.8956799071675|71936.18612274907|   NULL|96.65329873613035|     NULL|  NULL|10.487806451704587|2.8921743770496837|62397.40520238599|0.5816543579989917|0.45584046447513327|0.49979692845891815|57510.49281769821|0.40276858399486065|\n",
      "|    min|                 1|         15565701|  Abazu|              350|   France|Female|                18|                 0|              0.0|                 1|                  0|                  0|            11.58|                  0|\n",
      "|    max|             10000|         15815690| Zuyeva|              850|    Spain|  Male|                92|                10|        250898.09|                 4|                  1|                  1|        199992.48|                  1|\n",
      "+-------+------------------+-----------------+-------+-----------------+---------+------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1023d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-----------+---------+------+---+------+-------+-------------+---------+--------------+---------------+------+\n",
      "|RowNumber|CustomerId|Surname|CreditScore|Geography|Gender|Age|Tenure|Balance|NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|\n",
      "+---------+----------+-------+-----------+---------+------+---+------+-------+-------------+---------+--------------+---------------+------+\n",
      "|        0|         0|      0|          0|        0|     0|  0|     0|      0|            0|        0|             0|              0|     0|\n",
      "+---------+----------+-------+-----------+---------+------+---+------+-------+-------------+---------+--------------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_manquantes = df.select(\n",
    "    [sum(col(column).isNull().cast('int')).alias(column) for column in df.columns]\n",
    ")\n",
    "df_manquantes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00165fc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NumericType\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m numeric_cols = [column \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m.columns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(column.dataType, NumericType)]\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(numeric_cols)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "numeric_cols = [column for column in df.columns if isinstance(column.dataType, NumericType)]\n",
    "print(numeric_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e1d46",
   "metadata": {},
   "source": [
    "# pyspark.sql.DataFrame.approxQuantile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac4455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
